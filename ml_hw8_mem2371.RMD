---
title: 'Exercise & Assignment: Dealing with High Dimensional Data'
author: "JAS"
date: " "
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1: Feature selection using regularization methods

This exercise is *loosely* based on the following paper: Integration of an interpretable machine learning algorithm to identify early life risk factors of childhood obesity among preterm infants: a prospective birth cohort https://doi.org/10.1186/s12916-020-01642-6. The data used in this exercise are an altered version of data available from the HHEAR Data Center, with dois https://doi.org/10.36043/2017-1740_EPI_58 and https://doi.org/10.36043/2017-1740_MAIN_84


In this exercise, you will utilize the caret package to optimize a regularization algorithm for feature selection. You will compare results when you include variables that could induce confounding as features entered into the algorithm.  You will also consider how study design and source of data can impact the conclusions drawn by a machine learning analysis. 

***

### Description of the Theoretical Study and Data

The goal of this study is to identify prenatal social and environmental risk factors for childhood overweight/obesity among preterm infants.This study is a prospective birth cohort involving mother-child pairs. Women were enrolled during the first or second trimester of pregnancy and were followed up via visiting clinics until the birth of their children. Women and children were then followed up periodically during infancy and childhood. A total of 1447 singleton children were born preterm, prior to 37 weeks gestation and had complete data on maternal demographics and pregnancy, birth characteristics, lifestyle factors, biospecimen analyzed for exposure to metals and to define childhood obesity at age 5. You have recently been hired as a research data analyst, and tasked with performing the analysis for this study.You are provided with a dataset containing a number of features, in addition to a binary outcome indicating childhood overweight or obesity vs normal weight. 

Features in the dataset have informative names. The following categorical features use codes to indicate the different labels:

Child.Human.Biological.Sex: sex assigned at birth of child; 
110:Female
111:Male

HHIncome: Household income during pregnancy; 
159: <$5,000
204:$5,000-$10,000, 
205: $10,000-$20,000, 
206: $20,000-$40,000, 
207: $40,000-$70,000, 
208: >$70,000

Race_ethnicity: Race or ethnicity of child, as reported by parent;
47:"Hispanic or Latino Ethnicity"
54:"Multiracial"
210:"Black Non-Hispanic"
212:"American Indian Non-Hispanic"
214:"White Non-Hispanic"
217:"Asian Non-Hispanic"
855824:"Other race/ethnicity than white/black/hispanic/asian/american indian/multiracial"

Mother_Education: Highest Educational Attainment at time of Pregnancy
4:"Advanced Graduate Degree"
12:"College Graduate"
32:"Graduated From High School"
203:"Some College or Technical School"
215:"Less than High School"

Smoking_Preg: maternal smoking during pregnancy
1: No smoking during pregnancy
2: Active smoker during pregnancy
81: Quit smoking before pregnancy

ow_obesity: overweight or obesity during childhood
1: Overweight or Obese >= 85th percentile
0: Typical weight <85th percentile (no underweight children in sample)

***

###Before Data Analysis
Question 1: What additional information, if any, would you want from the principal study investigators in regards to the above features? 

We would want to obtain domain knowledge from the principal study investigators. They may have come across abnormalities within the data, or ways in which the study was carried out, that they would flag to other investigators. For example, they may be aware of ways in which the questions were posed which could bias the way the respondents answered and would want to flag those questions to respondents. As smoking during pregnancy is highly stigmatized, we would want to obtain information regarding how the question was asked in order to be able to gauge whether misclassification is of concern. They additionally may be able to advise regarding appropriate categorization or recoding of variables, as they have done in prior studies or have seen done in the literature.

In relation to other features in the dataset, we would need to know clinically relevant values, which the investigators could provide.

Question 2: Look at the features in the dataset before you start your analysis. Are there any you want to exclude from your analysis completely? Why or why not? Are there any you want to recode or transform? Why or why not?

To be able to answer whether we would want to recode some variables, we might want to begin data exploration and determine whether small cell sizes are of concern. We would likely want to recode the chemical concentration variables as greater than or equal to a concentration of clinical significance. 

We could also consider removing variables that we hypothesize to be highly correlated. For example, including both birth weight and birth length may be redundant, as they are likely to be correlated. If I had access to appropriate resources, I would want to recode all the concentration variables to be greater than or below clinically significant levels, as this would be more interpretable. We would also want to exclude variables that have a lot of missing data.

Question 3: Are any of the features not of interest as modifiable contributors to childhood overweight/obesity themselves, but in an explanatory model, you would typically include them? Will you include them in your analysis?

I would propose that birth length and gestational period would both be variables that could induce confounding if birth weight is already in the model. While in an explanatory model, we would perhaps want to include these variables, I would remove them both because having birth weight in the model should be sufficient for predicting childhood obesity. The height of the child could also be removed from the model, because height of the child in of itself will likely not be predictive of obesity, but rather is taken into account as a way of measuring obesity. I would also argue that sex would be considered a confounder of childhood obesity among preterm infants and should thus not be included.

### Step 1: Load Packages and Prepare Data
You will start by loading the needed packages. Some are already listed, but you can choose to use different ones. You will need to clean the data, check that values are plausible, ensure that all variables are the correct type for the algorithm and packages you want to use, etc.

```{r data_prep}
library(tidyverse)
library(caret)
library(glmnet)
library(Amelia)

#Importing and recoding the data to be appropriate for the variable type
bw.data = read_csv("./data/birthcohort_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    sex = as.factor(child_human_biological_sex),
    income = as.factor(hh_income),
    race = as.factor(race_ethnicity),
    education = as.factor(mother_education),
    smoking = as.factor(smoking_preg),
    obesity = as.factor(ow_obesity)
  ) %>% 
  select(
    -child_human_biological_sex,
    -hh_income,
    -race_ethnicity,
    -mother_education,
    -smoking_preg,
    -ow_obesity
    )

#Checking for missing values
missmap(bw.data)

#Excluding variables previously hypothesized to not contribute to obesity
bw.new = bw.data %>% 
  select(
    -child_height,
    -birth_length,
    -gestational_length,
    -sex
  )

#Checking whether data is unbalanced
summary(bw.new$obesity) %>% 
  knitr::kable()

#Outcome prevalence of 23.6%. Is this unbalanced?
```

### Step 2: Decide on a pipeline

Question 4: In previous exercises, we often partition our sample into training and testing. We optimize hyperparameters using cross-validation. Is this pipeline still necessary if our goal is feature selection and not building a prediction model to apply to new data? What do you think? 

Yes, we want to partition the data. We are using a data-driven pipeline, and by doing so, we need to be taking the right approaches to check how our algorithms are functioning. Optimizing hyperparameteres for feature selection, like varying the number of trees for example, can create notable changes in terms of variable importance. Partitioning the data in order to optimize these hyperparameters is needed, even if the research question is not related to prediction.

Regardless of your answer above, partition the data into a 70/30 split just to get the practice with the programming code to partition.

```{r partition}
set.seed(100)

train.indices = createDataPartition(y = bw.new$obesity,p = 0.7,list = FALSE)

training = bw.new[train.indices,]
testing = bw.new[-train.indices,]
```

### Step 3: Construct a model using a regularization algorithm and the features of interest in the training data

Question 5: Which regularization algorithm seems most appropriate for this research question? Justify your choice. 

I would use a LASSO algorithm. LASSO is highly interpretable, which is necessary when conducting feature selection, as we want to be able to distinguish and understand the features that have been selected; we want to be able to communicate that the selected features are probable risk factors. 

Question 6: Which metrics will you use to evaluate your model? Consider your research question and the outcome of interest. 

I will use accuracy as an evaluation metric to evaluate the model.

Assess how the metric(s) change(s) based on values of the hyperparameters. Construct a grid to explore various values (do not just use the default parameters). Once you have a final model, determine the features that are considered "important" based on the model output.
 
```{r}
set.seed(100)

#Create grid to search lambda
lambda = 10^seq(-3,3, length = 100)

lasso.bw = train(
  obesity ~., 
  data = training, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10), 
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso.bw$bestTune

#Print all of the options examined
lasso.bw$results

# Model coefficients
coef(lasso.bw$finalModel, lasso.bw$bestTune$lambda)
varImp(lasso.bw)

# Make predictions
pred.bw <- lasso.bw %>% predict(x.train)

pred.bw = predict(lasso.bw, training)
pred.bw.prob = predict(lasso.bw, training, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.bw, training$obesity, positive = "1")
print(eval.results)

#Accuracy of this model is 0.81
```

### Step 4: Test your final model in the testing dataset

Use the implementation of your model in the testing set to obtain final performance metrics and perform the inference needed to address your research question. 

Question 7: Summarize your final conclusion in 2-3 sentences

```{r}
set.seed(100)

# Using best fit model from above with testing data
pred.lasso.f = predict(lasso.bw, testing)
pred.lasso.f.prob = predict(lasso.bw, testing, type = "prob")

# Evaluating in testing data with confusion matrix
eval.results = confusionMatrix(pred.lasso.f, testing$obesity, positive = "1")
print(eval.results)
```

The final accuracy of this model is **80.37**. The variable that seems to be of the most importance is parental-reported race/ethnicity of the child being "other". This is closely followed by cobalt concentration in the mothers' blood during pregnancy, and race of the child being American Indian. Many of the variables that are predictors of childhood obesity are categories of race/ethinicity, as is income.

### Step 5: Construct another model, making a different choice of variable inclusion. 

Redo the above, but now make the opposite choice about variable inclusion. That is, if you did not include the features that themselves might not be modifiable contributors to childhod overweight/obesity, but you would typically include in an explanatory model, include them now. Conversely, if you include those variables previously, exclude them now

Question 7: Do the "important" features change when you make a different choice about the other features? Do the hyperparameters that optimize the model change when the additional variables are included? What about model performance?

```{r}
set.seed(100)

train.indices = createDataPartition(y = bw.data$obesity,p = 0.7,list = FALSE)

training.2 = bw.data[train.indices,]
testing.2 = bw.data[-train.indices,]

#Create grid to search lambda
lambda = 10^seq(-3,3, length = 100)

lasso.2 = train(
  obesity ~., 
  data = training.2, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10), 
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso.2$bestTune

#Print all of the options examined
lasso.2$results

# Model coefficients
coef(lasso.2$finalModel, lasso.2$bestTune$lambda)
varImp(lasso.2)

# Make predictions
pred.bw.2 = predict(lasso.2, training.2)
pred.bw.prob.2 = predict(lasso.2, training.2, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.bw.2, training.2$obesity, positive = "1")
print(eval.results)

#Accuracy within training dataset => 0.90

# Using best fit model from above with testing data
pred.lasso.f.2 = predict(lasso.2, testing.2)
pred.lasso.f.prob.2 = predict(lasso.2, testing.2, type = "prob")

# Evaluating in testing data with confusion matrix
eval.results = confusionMatrix(pred.lasso.f.2, testing.2$obesity, positive = "1")
print(eval.results)

#Accuracy within testing dataset => 0.87
```

The important features change with inclusion of the other variables, 

When the additional variables are included, lambda does change from 0.02 to 0.014. 

## Exercise 2: Creating more refined phenotypes for an explanatory analysis.

This exercise is *loosely* based on the following paper: Deploying unsupervised clustering analysis to derive clinical phenotypes and risk factors associated with mortality risk in 2022 critically ill patients with COVID-19 in Spain doi:10.1186/s13054-021-03487-8. Data were simulated and are not true COVID data.

Researchers are interested in understanding the factors associated with ICU mortality among COVID-19 patients. They hypothesize there are different clinical phenotypes that could be at different risks for mortality and require different medical interventions. The goal of this research is to determine if patient features including demographics and clinical data at ICU admission could be used to separate COVID-19 patients into distinct phenotypic clusters. The secondary aim was to determine if identified phenotypic clusters had different risk of mortality. 

You are provided with the following dataset for a subset of 178 COVID-19 patients, with the instructions to conduct an unsupervised analysis to identify phenotypic clusters within the patient population, describe the clusters in terms of the input features and then determine if there are differences in mortality rate across the clusters. All feature data in the dataset have been centered and scaled. The outcome, mortality, is a binary indicator.

Variables in the dataset (covid.csv) are:

Ageyr: the age of the patient at admission to the ICU
APACHE:APACHE II Score, Acute Physiology and Chronic Health Evaluation II Score
SOFA: SOFA Score Sequential Organ Failure Assessment
DDimer: D dimer, a fibrin degradation product, can indicate blood clot formation and breakdown
SerumLactate: measures level of lactic acid in the blood, can be indicator of hypoxia
Ferritin: measure of iron sufficiency in blood
CRP: C-reactive protein, measure of inflammation
Creatinine: product of protein metabolism, high levels can indicate impaired kidney function
WBC: concentration of white blood cells, marker of infection
DBP: diastolic blood pressure
Procalcitonin: marker of bacterial infection
IGA: immunoglobulin A, measure of antibodies found in mucous membranes
Oxmetric.1: measure of oxygen saturation
mortality: 1=Died in ICU, 0=Survived

***

### Step 1: Load needed libraries and implement the unsupervised analysis

Question 1: Name one potential risk/concern of obtaining data that has already been centered and scaled.

Question 2: What unsupervised analysis do you think is appropriate for this research question? Justify your answer.

Implement the unsupervised analysis you chose in Question 2, implementing appropriate analyses to determine the number of phenotypic clusters to keep within the analysis.

```{r}

```

### Step 2: Interpret the clusters

Describe the clusters in terms of both their input features and the incidence of mortality within the cluster.

```{r}

```

Question 3: A researcher at a different medical institution has heard about your analysis and is interested in using your results to determine risk of mortality within their ICU. What are some limitations or concerns of using the results from your unsupervised analysis in a different setting?







